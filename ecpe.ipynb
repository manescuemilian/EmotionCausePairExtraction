{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010dd5f0",
   "metadata": {},
   "source": [
    "# Imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81efc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and constants\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import multiprocessing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import random\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Attention, Dense, Conv1D, MaxPooling1D, LayerNormalization, ReLU\n",
    "from keras.layers import add, multiply\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, Add, BatchNormalization, Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts, CosineDecay\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import joblib\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing for training using GPU\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'Device found : {physical_device}')\n",
    "tf.config.experimental.set_memory_growth(physical_device[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd00acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validation and test set image folders\n",
    "TRAIN_FILE = 'train_dataset.json'\n",
    "TEST_FILE = 'test_dataset.json'\n",
    "FULL_DATASET_FILE = 'full_dataset.json'\n",
    "\n",
    "# Some constants related to model training\n",
    "BATCH_SIZE = 32\n",
    "NUM_EMOTIONS = 7\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d8f6b",
   "metadata": {},
   "source": [
    "# Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "def get_json_file(file_name):\n",
    "\twith open(file_name) as f:\n",
    "\t\td = json.load(f)\n",
    "\t\treturn d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = get_json_file(TRAIN_FILE)\n",
    "test_json = get_json_file(TEST_FILE)\n",
    "\n",
    "train_conversations =  train_json[\"conversation\"]\n",
    "test_conversations = test_json[\"conversation\"]\n",
    "\n",
    "train_pairs = train_json[\"emotion-cause_pairs\"]\n",
    "test_pairs = test_json[\"emotion-cause_pairs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d7987",
   "metadata": {},
   "source": [
    "## Cleanup text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove english stopwords from text\n",
    "def remove_stopwords(text, stop_words):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Lemmatize text (reduce words coming from the same stem to a single word)\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "# Function for cleaning a given piece of text\n",
    "def cleanup_text(text, apply_lemmatization=False):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Fix contractions (you're => you are)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation and whitespaces\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Lemmatizer\n",
    "    if apply_lemmatization:\n",
    "        text = lemmatize_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocess the \"text\" column in a dataframe\n",
    "def preprocess_dataframe(df):\n",
    "     df['text'] = df['text'].apply(lambda x: cleanup_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b46b7",
   "metadata": {},
   "source": [
    "## Get dataset of utterances (one utterance = one element of the datset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataframe in which one row = one utterance of the dataset, without taking conversations into consideration\n",
    "def get_dataframe_utterances(conversations):\n",
    "\tconversation_dfs = []\n",
    "\tfor _, conversation_data in conversations.items():\n",
    "\t\tfor utterance_data in conversation_data:\n",
    "\t\t\tnew_conversations = {key: utterance_data[key] for key in [\"text\", \"emotion\"]}\n",
    "\t\t\tconversation_dfs.append(pd.DataFrame([new_conversations]))\n",
    "\treturn pd.concat(conversation_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utterances_df_initial = get_dataframe_utterances(train_conversations)\n",
    "test_utterances_df = get_dataframe_utterances(test_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataframe(train_utterances_df_initial)\n",
    "preprocess_dataframe(test_utterances_df)\n",
    "print(train_utterances_df_initial.head(5))\n",
    "print()\n",
    "print(test_utterances_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d93e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_utterances_df, validation_utterances_df = train_test_split(train_utterances_df_initial, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f055317",
   "metadata": {},
   "source": [
    "## Get conversation dataset (one conversation = one element of the dateset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute maximum conversation length from the dataset\n",
    "def get_max_conversation_length(train_conversations, test_conversations):\n",
    "\tmax_train_len = max([len(conversation) for conversation in train_conversations.values()])\n",
    "\tmax_test_len = max([len(conversation) for conversation in test_conversations.values()])\n",
    "\treturn max(max_train_len, max_test_len)\n",
    "\n",
    "max_conversation_length = get_max_conversation_length(train_conversations, test_conversations)\n",
    "print(f\"Max conversation length is {max_conversation_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70297be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_text_train = [[cleanup_text(utterance[\"text\"]) for utterance in conv] for conv in train_conversations.values()]\n",
    "utterances_text_test = [[cleanup_text(utterance[\"text\"]) for utterance in conv] for conv in test_conversations.values()]\n",
    "\n",
    "tokenizer_conversation = Tokenizer()\n",
    "tokenizer_conversation.fit_on_texts([\" \".join(conv) for conv in utterances_text_train])\n",
    "\n",
    "sequences_conv_train = []\n",
    "for conv in utterances_text_train:\n",
    "\tsequence = tokenizer_conversation.texts_to_sequences(conv)\n",
    "\tsequences_conv_train.append(sequence)\n",
    "\n",
    "sequences_conv_test = []\n",
    "for conv in utterances_text_test:\n",
    "\tsequence = tokenizer_conversation.texts_to_sequences(conv)\n",
    "\tsequences_conv_test.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f22784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum sequence length from the dataset\n",
    "def get_max_sequence_length(sequences_train, sequences_test):\n",
    "\tmax_train_seq = max([max([len(utt) for utt in conv]) for conv in sequences_train])\n",
    "\tmax_test_seq = max([max([len(utt) for utt in conv]) for conv in sequences_test])\n",
    "\treturn max(max_train_seq, max_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = get_max_sequence_length(sequences_conv_train, sequences_conv_test)\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder_conversations = LabelEncoder()\n",
    "label_encoder_conversations = label_encoder_conversations.fit(train_utterances_df['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataset of size (number_conversations, max_conversation_length, max_sequence_length)\n",
    "def get_dataset_conversations(sequences, max_conversation_length, max_sequence_length):\n",
    "\tnum_conversations = len(sequences)\n",
    "\tpadded_sequences = [pad_sequences(seq, maxlen=max_sequence_length, padding='post', truncating='post') for seq in sequences]\n",
    "\tinput_data = np.zeros((num_conversations, max_conversation_length, max_sequence_length))\n",
    "\tfor i, sequence in enumerate(padded_sequences):\n",
    "\t\tfor j, seq in enumerate(sequence):\n",
    "\t\t\tinput_data[i, j, :] = seq\n",
    "\treturn input_data\n",
    "\n",
    "# Get labels (emotion labels and cause labels) from the datset (will be used for training)\n",
    "def get_labels_conv(conversation_json, max_conversation_length, label_encoder):\n",
    "\tconversations_dataset = conversation_json['conversation']\n",
    "\tnum_conversations = len(conversations_dataset)\n",
    "\n",
    "\t# Emotion labels\n",
    "\tlabels_emotion = np.zeros((num_conversations, max_conversation_length, 1))\n",
    "\tfor i, conv in enumerate(conversations_dataset.values()):\n",
    "\t\tfor j, utterance in enumerate(conv):\n",
    "\t\t\tlabels_emotion[i, j, :] = label_encoder.transform([utterance[\"emotion\"]])\n",
    "\tlabels_emotion = to_categorical(labels_emotion, NUM_EMOTIONS)\n",
    "\n",
    "\t# Cause labels\n",
    "\tlabels_causes = np.zeros((num_conversations, max_conversation_length, 1))\n",
    "\tconversations_pairs = conversation_json['emotion-cause_pairs']\n",
    "\tfor i, conv_pairs in enumerate(conversations_pairs.values()):\n",
    "\t\tfor pair in conv_pairs:\n",
    "\t\t\tcause = pair[1]\n",
    "\t\t\tcause_id = int(cause.split(\"_\")[0]) - 1\n",
    "\t\t\tlabels_causes[i, cause_id, :] = 1\n",
    "\treturn labels_emotion, labels_causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_conv_emotions, y_train_conv_causes = get_labels_conv(train_json, max_conversation_length, label_encoder_conversations)\n",
    "y_test_conv_emotions, y_test_conv_causes = get_labels_conv(test_json, max_conversation_length, label_encoder_conversations)\n",
    "\n",
    "X_train_conv_init = get_dataset_conversations(sequences_conv_train, max_conversation_length, max_sequence_length)\n",
    "\n",
    "X_train_conv, X_validation_conv, \\\n",
    "y_train_conv_emotions, y_validation_conv_emotions, \\\n",
    "y_train_conv_causes, y_validation_conv_causes, \\\n",
    "indices_conv_train, indices_conv_val = train_test_split(X_train_conv_init, y_train_conv_emotions, y_train_conv_causes, range(len(X_train_conv_init)),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.1, random_state=SEED)\n",
    "\n",
    "X_test_conv = get_dataset_conversations(sequences_conv_test, max_conversation_length, max_sequence_length)\n",
    "indices_conv_test = range(len(X_test_conv))\n",
    "\n",
    "print(f\"X_train shape = {X_train_conv.shape}; y_train_emotions shape = {y_train_conv_emotions.shape}, y_train_causes shape = {y_train_conv_causes.shape}\")\n",
    "print(f\"X_val shape = {X_validation_conv.shape}; y_val_emotions shape = {y_validation_conv_emotions.shape}, y_val_causes shape = {y_validation_conv_causes.shape}\")\n",
    "print(f\"X_test shape = {X_test_conv.shape}; y_test emotions shape = {y_test_conv_emotions.shape}, y_test causes shape = {y_test_conv_causes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6b47b",
   "metadata": {},
   "source": [
    "## Label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ed839",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x='emotion', data=train_utterances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94840c9",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_utterances_df_initial['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24278b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To categorical labels\n",
    "def get_categorical_labels(df, labels_column):\n",
    "\treturn pd.get_dummies(df[labels_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_utterances_df['text'])\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_utterances_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_utterances_df['text'])\n",
    "\n",
    "max_length = max(max(map(len, train_sequences)), max(map(len, validation_sequences)), max(map(len, test_sequences)))\n",
    "print(f\"Max length = {max_length}\")\n",
    "\n",
    "# Compute the X and y values for the subtask 0\n",
    "X_train_task0 = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "y_train_task0 = get_categorical_labels(train_utterances_df, \"emotion\")\n",
    "\n",
    "X_val_task0 = pad_sequences(validation_sequences, maxlen=max_length, padding='post')\n",
    "y_val_task0 = get_categorical_labels(validation_utterances_df, \"emotion\")\n",
    "\n",
    "X_test_task0 = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "y_test_task0 = get_categorical_labels(test_utterances_df, \"emotion\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "print(f\"Vocab size is {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7462b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute y values and class weights\n",
    "y_train = train_utterances_df['emotion']\n",
    "y_val = validation_utterances_df['emotion']\n",
    "y_test = test_utterances_df['emotion']\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {class_label: weight for class_label, weight in zip(np.arange(NUM_EMOTIONS), class_weights)}\n",
    "\n",
    "class_weight_dict_labels = {class_label: weight for class_label, weight in zip(np.unique(y_train), class_weights)}\n",
    "\n",
    "print(class_weight_dict_labels)\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e06a8c",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ada02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "embedding_matrix_word2vec = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix_word2vec[i] = word2vec_model[word]\n",
    "        \n",
    "print(embedding_matrix_word2vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff57f6",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d507043",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "embedding_matrix_fasttext = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix_fasttext[i] = ft.get_word_vector(word)\n",
    "\n",
    "print(embedding_matrix_fasttext.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c661e",
   "metadata": {},
   "source": [
    "### Universal sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82796e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = embed(train_utterances_df['text'])\n",
    "X_val_embed = embed(validation_utterances_df['text'])\n",
    "X_test_embed = embed(test_utterances_df['text'])\n",
    "\n",
    "print(f\"Train shape = {X_train_embed.shape}; Validation shape = {X_val_embed.shape}; Test shape = {X_test_embed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014374f2",
   "metadata": {},
   "source": [
    "# Task 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384df0aa",
   "metadata": {},
   "source": [
    "## Emotion Extraction in Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for subtask 0\n",
    "def compile_model(model, num_epochs, steps_per_epoch, epoch_decay_rate=0.9, \n",
    "\t\t\t\t  use_cosine_decay=False, use_cosine_decay_restarts=False, warmup_epochs=10, initial_learning_rate=0.01, final_learning_rate=0.0001,\n",
    "\t\t\t\t  loss_fn = None, label_smoothing=0, weight_decay=0):\n",
    "\tif use_cosine_decay:\n",
    "\t\tprint(f\"Using Cosine Decay; label_smoothing = {label_smoothing} and weight_decay={weight_decay}\")\n",
    "\t\talpha = final_learning_rate / initial_learning_rate\n",
    "\t\tfirst_decay_steps = np.floor(steps_per_epoch * num_epochs * epoch_decay_rate)\n",
    "\t\tprint(f\"first_decay_steps = {first_decay_steps}\")\n",
    "\t\twarmup_steps = steps_per_epoch * warmup_epochs\n",
    "\t\tlr_schedule = CosineDecay(0.0, first_decay_steps, alpha, warmup_target=initial_learning_rate, warmup_steps=warmup_steps)\n",
    "\t\toptimizer = Adam(learning_rate=lr_schedule, decay=weight_decay)\n",
    "\telif use_cosine_decay_restarts:\n",
    "\t\talpha = final_learning_rate / initial_learning_rate\n",
    "\t\tfirst_decay_steps = steps_per_epoch * num_epochs * 0.1\n",
    "\t\tprint(f\"Using Cosine Decay with Restarts; label_smoothing = {label_smoothing} and weight_decay={weight_decay}\")\n",
    "\t\tprint(f\"Alpha = {alpha} for epochs = {num_epochs * 0.1}\")\n",
    "\t\tlr_schedule = CosineDecayRestarts(initial_learning_rate=initial_learning_rate, alpha=alpha, first_decay_steps=first_decay_steps)\n",
    "\t\toptimizer=Adam(learning_rate=lr_schedule, decay=weight_decay)\n",
    "\telse:\n",
    "\t\tprint(\"Using default Adam optimizer\")\n",
    "\t\toptimizer = Adam(clipnorm=1.0)\n",
    "\n",
    "\tif loss_fn is None:\n",
    "\t\tloss_fn = CategoricalCrossentropy(label_smoothing=label_smoothing)\n",
    "\n",
    "\t# Compile the model\n",
    "\tmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Fit model for subtask 0\n",
    "def fit_model(model, X_train, y_train, X_val, y_val, num_epochs=40, checkpoint_name='best_model.h5', class_weight=None):\n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_name, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "    model.fit(X_train, y_train,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=num_epochs,\n",
    "              workers=multiprocessing.cpu_count(),\n",
    "              callbacks=[checkpoint_callback],\n",
    "\t\t\t  class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps_per_epoch(X, batch_size=BATCH_SIZE):\n",
    "\tnum_samples = len(X)\n",
    "\treturn np.ceil(num_samples / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653187a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_task0 = get_steps_per_epoch(X_train_task0, BATCH_SIZE)\n",
    "print(steps_per_epoch_task0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c517305",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e61665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model RNN1\n",
    "def get_emotion_extraction_model1(embedding_matrix, embedding_dim=300, max_length=30, activation='relu'):\n",
    "\t# Build the model\n",
    "\tX_input = Input((max_length, ))\n",
    "\n",
    "\t# Embedding layer with pretrained Word2Vec features\n",
    "\tX = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_dim,\n",
    "\t\t\t   \t  weights=[embedding_matrix], input_length=max_length,\n",
    "\t\t\t\t  trainable=False, mask_zero=True)(X_input)\n",
    "\tX = Dropout(0.2)(X)\n",
    "\t\n",
    "\tX = Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))(X)\n",
    "\tX = LayerNormalization()(X)\n",
    "\tX = Attention(max_length)([X, X])\n",
    "\n",
    "\tX = Bidirectional(LSTM(256, dropout=0.2, return_sequences=True))(X)\n",
    "\tX = LayerNormalization()(X)\n",
    "\n",
    "\tX = Bidirectional(LSTM(256, dropout=0.2, return_sequences=False))(X)\n",
    "\tX = LayerNormalization()(X)\n",
    "\n",
    "\tX = Dense(128, activation=activation)(X)\n",
    "\tX = Dense(64, activation=activation)(X)\n",
    "\tX = Dropout(0.2)(X)\n",
    "\n",
    "\t# Output layer for the emotions\n",
    "\tX = Dense(NUM_EMOTIONS, activation='softmax')(X)\n",
    "\n",
    "\tmodel = Model(inputs=X_input, outputs=X)\n",
    "\t\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73176a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for printing the classification report for RNN1 model\n",
    "def get_model_1_results(model, weights_path, X, y):\n",
    "\tmodel.load_weights(weights_path)\n",
    "\ty_pred = model.predict(X)\n",
    "\ty_pred = np.argmax(y_pred, axis=-1)\n",
    "\ty_true = np.argmax(y, axis=-1)\n",
    "\tprint(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e69305",
   "metadata": {},
   "source": [
    "#### Model 1 with Word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b388b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_default_word2vec = get_emotion_extraction_model1(embedding_matrix_word2vec, embedding_dim, max_length)\n",
    "compile_model(model_1_default_word2vec, 100, steps_per_epoch_task0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ceb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(model_1_default_word2vec, X_train_task0, y_train_task0, X_val_task0, y_val_task0, 100, 'rnn_1_default_word2vec.h5', class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_1_results(model_1_default_word2vec, \"models/rnn_1_default_word2vec.h5\", X_val_task0, y_val_task0)\n",
    "get_model_1_results(model_1_default_word2vec, \"models/rnn_1_default_word2vec.h5\", X_test_task0, y_test_task0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_restarts_word2vec = get_emotion_extraction_model1(embedding_matrix_word2vec, embedding_dim, max_length)\n",
    "compile_model(model_1_restarts_word2vec, 100, steps_per_epoch_task0, use_cosine_decay_restarts=True, label_smoothing=0.1, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(model_1_restarts_word2vec, X_train_task0, y_train_task0, X_val_task0, y_val_task0, 100, 'rnn_1_restarts_word2vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_1_results(model_1_restarts_word2vec, \"models/rnn_1_restarts_word2vec.h5\", X_val_task0, y_val_task0)\n",
    "get_model_1_results(model_1_restarts_word2vec, \"models/rnn_1_restarts_word2vec.h5\", X_test_task0, y_test_task0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99132cc9",
   "metadata": {},
   "source": [
    "#### Model 1 with FastText features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_default_fasttext = get_emotion_extraction_model1(embedding_matrix_fasttext, embedding_dim, max_length)\n",
    "compile_model(model_1_default_fasttext, 100, steps_per_epoch_task0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f475f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_1_results(model_1_default_fasttext, \"models/rnn_1_default_fasttext.h5\", X_val_task0, y_val_task0)\n",
    "get_model_1_results(model_1_default_fasttext, \"models/rnn_1_default_fasttext.h5\", X_test_task0, y_test_task0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b723df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(model_1_default_fasttext, X_train_task0, y_train_task0, X_val_task0, y_val_task0, 100, 'rnn_1_default_fasttext.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8730138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_fasttext = get_emotion_extraction_model1(embedding_matrix_fasttext, embedding_dim, max_length)\n",
    "compile_model(model_1_fasttext, 100, steps_per_epoch_task0, use_cosine_decay_restarts=True, label_smoothing=0.1, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4263fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(model_1_fasttext, X_train_task0, y_train_task0, X_val_task0, y_val_task0, 100, 'rnn_1_restarts_fasttext.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_1_results(model_1_fasttext, \"models/rnn_1_restarts_fasttext.h5\", X_val_task0, y_val_task0)\n",
    "get_model_1_results(model_1_fasttext, \"models/rnn_1_restarts_fasttext.h5\", X_test_task0, y_test_task0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cba5a9",
   "metadata": {},
   "source": [
    "### Model 2 (using machine learning models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a266427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply embedding matrix to the sequences\n",
    "def transform_sequences_with_embedding(sequence, embedding_matrix):\n",
    "    if np.all(sequence == 0) or len(sequence) == 0:\n",
    "        return np.zeros(embedding_matrix.shape[1])\n",
    "    return np.mean(embedding_matrix[sequence], axis=0)\n",
    "\n",
    "# Get datasets for the training of Machine Learning classic models\n",
    "def get_datasets_ml(train_sequences, validation_sequences, test_sequences, embedding_matrix):\n",
    "    X_train = np.array([transform_sequences_with_embedding(seq, embedding_matrix) for seq in train_sequences])\n",
    "    X_val = np.array([transform_sequences_with_embedding(seq, embedding_matrix) for seq in validation_sequences])\n",
    "    X_test = np.array([transform_sequences_with_embedding(seq, embedding_matrix) for seq in test_sequences])\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "# Scale the datasets\n",
    "def scale_sets(X_train, X_val, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "    X_test_scale = scaler.transform(X_test)\n",
    "    return X_train_scale, X_val_scale, X_test_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ml_w2v, X_val_ml_w2v, X_test_ml_w2v = get_datasets_ml(train_sequences, validation_sequences, test_sequences, embedding_matrix_word2vec)\n",
    "X_train_ml_w2v, X_val_ml_w2v, X_test_ml_w2v = scale_sets(X_train_ml_w2v, X_val_ml_w2v, X_test_ml_w2v)\n",
    "print(f\"Train shape = {X_train_ml_w2v.shape}, val shape = {X_val_ml_w2v.shape}, test shape = {X_test_ml_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cffc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ml_ft, X_val_ml_ft, X_test_ml_ft = get_datasets_ml(train_sequences, validation_sequences, test_sequences, embedding_matrix_fasttext)\n",
    "X_train_ml_ft, X_val_ml_ft, X_test_ml_ft = scale_sets(X_train_ml_ft, X_val_ml_ft, X_test_ml_ft)\n",
    "print(f\"Train shape = {X_train_ml_ft.shape}, val shape = {X_val_ml_ft.shape}, test shape = {X_test_ml_ft.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels in numerical form\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_boost = label_encoder.fit_transform(y_train)\n",
    "y_val_boost = label_encoder.transform(y_val)\n",
    "y_test_boost = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c770d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LGBM classifier and obtain classification report for the validation and test set\n",
    "def train_lgbm(X_train, y_train, X_val, y_val, X_test, y_test, reg_lambda=10, reg_alpha=0.1):\n",
    "    lgbm_params = {\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"n_estimators\": 700,\n",
    "        \"subsample\": 0.5,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"objective\": 'multiclass',\n",
    "        'num_class': 7,\n",
    "        \"num_leaves\": 20,\n",
    "        \"random_state\": SEED,\n",
    "    }\n",
    "\n",
    "    lgbm_clf = lgbm.LGBMClassifier(**lgbm_params)\n",
    "    lgbm_clf.fit(X_train, y_train)\n",
    "    joblib.dump(lgbm_clf, \"lgbm_model.joblib\")\n",
    "\n",
    "    train_acc = lgbm_clf.score(X_train, y_train)\n",
    "    val_acc = lgbm_clf.score(X_val, y_val)\n",
    "    print(f\"Train acc = {train_acc}; val_acc = {val_acc}\")\n",
    "\n",
    "    print(classification_report(y_val, lgbm_clf.predict(X_val)))\n",
    "    print(classification_report(y_test, lgbm_clf.predict(X_test)))\n",
    "\n",
    "    return lgbm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316140ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm(X_train_ml_w2v, y_train_boost, X_val_ml_w2v, y_val_boost, X_test_ml_w2v, y_test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc13ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm(X_train_ml_ft, y_train_boost, X_val_ml_ft, y_val_boost, X_test_ml_ft, y_test_boost, reg_lambda=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm(X_train_embed, y_train_boost, X_val_embed, y_val_boost, X_test_embed, y_test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model and obtain the classification report for the validation and test set\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\tmodel = xgb.XGBClassifier(n_estimators=300, learning_rate=0.1, \n",
    "\t\t\t\t\t\t\tgamma=0, max_depth=7, num_class=7, \n",
    "\t\t\t\t\t\t\tobjective=\"multiclass\", subsample=0.5, \n",
    "\t\t\t\t\t\t\treg_lambda=15, colsample_bytree=0.6,\n",
    "\t\t\t\t\t\t\trandom_state=SEED)\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\n",
    "\ttrain_acc = model.score(X_train, y_train)\n",
    "\tval_acc = model.score(X_val, y_val)\n",
    "\tprint(f\"Train acc = {train_acc}; val_acc = {val_acc}\")\n",
    "\n",
    "\tprint(classification_report(y_val, model.predict(X_val)))\n",
    "\tprint(classification_report(y_test, model.predict(X_test)))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f35ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(X_train_ml_w2v, y_train_boost, X_val_ml_w2v, y_val_boost, X_test_ml_w2v, y_test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2238d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(X_train_ml_ft, y_train_boost, X_val_ml_ft, y_val_boost, X_test_ml_ft, y_test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(X_train_embed, y_train_boost, X_val_embed, y_val_boost, X_test_embed, y_test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM model and obtain the classification report for the validation and test set\n",
    "def train_svm(X_train, y_train, X_val, y_val, X_test, y_test, kernel='rbf', C=1.0, class_weight=None):\n",
    "\tsvc_model = SVC(kernel=kernel, C=C, class_weight=class_weight, random_state=SEED)\n",
    "\tsvc_model.fit(X_train, y_train)\n",
    "\n",
    "\ty_train_pred = svc_model.predict(X_train)\n",
    "\ty_val_pred = svc_model.predict(X_val)\n",
    "\ty_test_pred = svc_model.predict(X_test)\n",
    "\n",
    "\t# Evaluate the performance\n",
    "\ttrain_acc = accuracy_score(y_train, y_train_pred)\n",
    "\tval_acc = accuracy_score(y_val, y_val_pred)\n",
    "\tprint(f\"Train_acc = {train_acc}; Validation Accuracy: {val_acc}\")\n",
    "\n",
    "\tprint(classification_report(y_val, y_val_pred))\n",
    "\tprint(classification_report(y_test, y_test_pred))\n",
    "\treturn svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e6890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(X_train_ml_w2v, y_train_boost, X_val_ml_w2v, y_val_boost, X_test_ml_w2v, y_test_boost, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(X_train_ml_ft, y_train_boost, X_val_ml_ft, y_val_boost, X_test_ml_ft, y_test_boost, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm(X_train_embed, y_train_boost, X_val_embed, y_val_boost, X_test_embed, y_test_boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807a54b",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739afdee",
   "metadata": {},
   "source": [
    "## Textual Emotion-Cause Pair Extraction in Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_task1 = get_steps_per_epoch(X_train_conv, BATCH_SIZE)\n",
    "print(steps_per_epoch_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for task 1\n",
    "def compile_model_task1(model, num_epochs, steps_per_epoch, epoch_decay_rate=0.9, \n",
    "\t\t\t\t\t\tuse_cosine_decay=False, use_cosine_decay_restarts=False, warmup_epochs=10, initial_learning_rate=0.01, final_learning_rate=0.0001,\n",
    "\t\t\t\t\t\tlabel_smoothing=0, weight_decay=0):\n",
    "\tif use_cosine_decay:\n",
    "\t\tprint(f\"Using Cosine Decay; label_smoothing = {label_smoothing} and weight_decay={weight_decay}\")\n",
    "\t\talpha = final_learning_rate / initial_learning_rate\n",
    "\t\tfirst_decay_steps = np.floor(steps_per_epoch * num_epochs * epoch_decay_rate)\n",
    "\t\tprint(f\"first_decay_steps = {first_decay_steps}\")\n",
    "\t\twarmup_steps = steps_per_epoch * warmup_epochs\n",
    "\t\tlr_schedule = CosineDecay(0.0, first_decay_steps, alpha, warmup_target=initial_learning_rate, warmup_steps=warmup_steps)\n",
    "\t\toptimizer = Adam(learning_rate=lr_schedule, decay=weight_decay)\n",
    "\telif use_cosine_decay_restarts:\n",
    "\t\talpha = final_learning_rate / initial_learning_rate\n",
    "\t\tfirst_decay_steps = steps_per_epoch * num_epochs * 0.1\n",
    "\t\tprint(f\"Using Cosine Decay with Restarts; label_smoothing = {label_smoothing} and weight_decay={weight_decay}\")\n",
    "\t\tprint(f\"Alpha = {alpha} for epochs = {num_epochs * 0.1}\")\n",
    "\t\tlr_schedule = CosineDecayRestarts(initial_learning_rate=initial_learning_rate, alpha=alpha, first_decay_steps=first_decay_steps)\n",
    "\t\toptimizer = Adam(learning_rate=lr_schedule, decay=weight_decay)\n",
    "\telse:\n",
    "\t\tprint(\"Using default Adam optimizer\")\n",
    "\t\toptimizer = Adam(clipnorm=1.0)\n",
    "\n",
    "\tlosses = {\"emotion_output\": \"categorical_crossentropy\", \"cause_output\": \"binary_crossentropy\"}\n",
    "\n",
    "\tloss_weights = {\"emotion_output\": 1.0, \"cause_output\": 1.0}\n",
    "\n",
    "\t# Compile the model\n",
    "\tmodel.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights, metrics=['accuracy'])\n",
    "\n",
    "# Fit model for task 1\n",
    "def fit_model_task1(model, X_train, y_train_emotions, y_train_causes, X_val, y_val_emotions, \n",
    "\t\t\t\t\ty_val_causes, num_epochs=40, checkpoint_name='best_model.h5', class_weight=None):\n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_name, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "    model.fit(X_train, y={\"emotion_output\": y_train_emotions, \"cause_output\": y_train_causes},\n",
    "              validation_data=(X_val, {\"emotions_output\": y_val_emotions, \"cause_output\": y_val_causes}),\n",
    "              epochs=num_epochs,\n",
    "              workers=multiprocessing.cpu_count(),\n",
    "              callbacks=[checkpoint_callback],\n",
    "\t\t\t  class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfd1d2",
   "metadata": {},
   "source": [
    "### Emotion and Cause classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted attention layer, which sums the values across the sequence axis\n",
    "class WeightedAttention(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(WeightedAttention, self).__init__()\n",
    "        self.hidden_dim = input_dim\n",
    "        self.projection = Sequential([Dense(hidden_dim), ReLU(), Dense(1)])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        projected_result = self.projection(inputs)\n",
    "        weights = tf.nn.softmax(projected_result, axis=1)\n",
    "        outputs = tf.reduce_sum(inputs * weights, axis=1)\n",
    "        return outputs, weights\n",
    "\n",
    "# LSTM-based encoder for a sequence\n",
    "class LSTMEncoder(keras.Model):\n",
    "    def __init__(self, lstm_hidden_dim):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.attention = WeightedAttention(lstm_hidden_dim, lstm_hidden_dim * 2)\n",
    "        self.lstm = Bidirectional(LSTM(lstm_hidden_dim // 2, return_sequences=True))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape is (batch, max_sequence_length)\n",
    "        output = self.lstm(inputs)\n",
    "        # output shape is (batch, max_sequence_length, lstm_hidden_dim)\n",
    "        output, _ = self.attention(output)\n",
    "        # out shape is (batch, lstm_hidden_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21609185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two branch (emotion and cause) classifier\n",
    "def get_emotion_cause_classifier(embedding_dim, vocab_size, lstm_hidden_dim, \n",
    "\t\t\t\t\t\t\t\t max_conversation_length, max_sequence_length, embedding_matrix,\n",
    "\t\t\t\t\t\t\t\t embedding_dropout=0.0):\n",
    "\tinputs_init = Input((max_conversation_length, max_sequence_length))\n",
    "\tinputs = tf.reshape(inputs_init, (-1, max_sequence_length))\n",
    "\n",
    "\tinputs = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False, mask_zero=True)(inputs)\n",
    "\tinputs = Dropout(embedding_dropout)(inputs)\n",
    "\n",
    "\t# Encoding for the case branch\n",
    "\tcause_encoding = LSTMEncoder(lstm_hidden_dim)(inputs)\n",
    "\tcause_encoding = tf.reshape(cause_encoding, (-1, max_conversation_length, lstm_hidden_dim))\n",
    "\tcause_encoding = Bidirectional(LSTM(lstm_hidden_dim // 2, return_sequences=True, batch_input_shape=(None, max_sequence_length, embedding_dim)))(cause_encoding)\n",
    "\n",
    "\t# Classify whether it is a cause or not\n",
    "\tcause_logits = Dense(1, activation='sigmoid', name='cause_output')(cause_encoding)\n",
    "\n",
    "\t# Encoding for the emotion branch\n",
    "\temotion_encoding = LSTMEncoder(lstm_hidden_dim)(inputs)\n",
    "\temotion_encoding = tf.reshape(emotion_encoding, (-1, max_conversation_length, lstm_hidden_dim))\n",
    "\temotion_encoding = tf.concat([emotion_encoding, cause_logits], axis=-1)\n",
    "\temotion_encoding = Bidirectional(LSTM(lstm_hidden_dim // 2, return_sequences=True, batch_input_shape=(None, max_sequence_length, embedding_dim + 2)))(emotion_encoding)\n",
    "\n",
    "\t# Classify whether it is emotion or not\n",
    "\temotion_logits = Dense(NUM_EMOTIONS, activation='softmax', name='emotion_output')(emotion_encoding)\n",
    "\n",
    "\treturn Model(inputs=inputs_init, outputs=[cause_logits, emotion_logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_argmax(array):\n",
    "    # Reshape to (batch_size * sequence_length, num_classes)\n",
    "    reshaped_array = array.reshape(-1, array.shape[-1])\n",
    "    \n",
    "    # Apply argmax along the last axis (assuming one-hot encoding)\n",
    "    argmax_result = np.argmax(reshaped_array, axis=1)\n",
    "    \n",
    "    return argmax_result\n",
    "\n",
    "def reshape_and_round(array):\n",
    "    # Reshape to (batch_size * sequence_length, num_classes)\n",
    "    reshaped_array = array.reshape(-1, array.shape[-1])\n",
    "    \n",
    "    # Apply argmax along the last axis (assuming one-hot encoding)\n",
    "    round_result = np.round(reshaped_array)\n",
    "    \n",
    "    return round_result\n",
    "\n",
    "def get_predictions(model, X):\n",
    "      return model.predict(X, batch_size=16)\n",
    "\n",
    "def get_metrics_task1(y_pred_causes, y_pred_emotions, y_causes, y_emotions):\n",
    "    print(\"Emotion classifier branch\")\n",
    "    print(classification_report(reshape_and_argmax(y_emotions), reshape_and_argmax(y_pred_emotions)))\n",
    "\n",
    "    print(\"Cause classifer branch\")\n",
    "    print(classification_report(reshape_and_round(y_causes), reshape_and_round(y_pred_causes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emotion_cause_classifier_restarts = get_emotion_cause_classifier(300, len(word_index) + 1, 50, max_conversation_length, max_sequence_length, embedding_matrix_word2vec, 0.2)\n",
    "model_emotion_cause_classifier_default = get_emotion_cause_classifier(300, len(word_index) + 1, 50, max_conversation_length, max_sequence_length, embedding_matrix_word2vec, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c394df",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model_task1(model_emotion_cause_classifier_default, 100, steps_per_epoch_task1)\n",
    "compile_model_task1(model_emotion_cause_classifier_restarts, 100, steps_per_epoch_task1, use_cosine_decay_restarts=True, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c717ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model_task1(model_emotion_cause_classifier_default, \n",
    "\t\t\t\tX_train_conv, y_train_conv_emotions, y_train_conv_causes, \n",
    "\t\t\t\tX_validation_conv, y_validation_conv_emotions, y_validation_conv_causes, \n",
    "\t\t\t\t100, 'emotions_cause_default_word2vec.h5')\n",
    "\n",
    "fit_model_task1(model_emotion_cause_classifier_restarts, \n",
    "\t\t\t\tX_train_conv, y_train_conv_emotions, y_train_conv_causes, \n",
    "\t\t\t\tX_validation_conv, y_validation_conv_emotions, y_validation_conv_causes, \n",
    "\t\t\t\t100, 'emotions_cause_restarts_word2vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec models\n",
    "model_emotion_cause_classifier_restarts.load_weights(\"models/emotions_cause_restarts_word2vec.h5\")\n",
    "model_emotion_cause_classifier_default.load_weights(\"models/emotions_cause_default_word2vec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c49175",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_causes, y_val_pred_emotions = get_predictions(model_emotion_cause_classifier_default, X_validation_conv)\n",
    "get_metrics_task1(y_val_pred_causes, y_val_pred_emotions, y_validation_conv_causes, y_validation_conv_emotions)\n",
    "\n",
    "y_test_pred_causes, y_test_pred_emotions = get_predictions(model_emotion_cause_classifier_default, X_test_conv)\n",
    "get_metrics_task1(y_test_pred_causes, y_test_pred_emotions, y_test_conv_causes, y_test_conv_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_causes, y_val_pred_emotions = get_predictions(model_emotion_cause_classifier_restarts, X_validation_conv)\n",
    "get_metrics_task1(y_val_pred_causes, y_val_pred_emotions, y_validation_conv_causes, y_validation_conv_emotions)\n",
    "\n",
    "y_test_pred_causes, y_test_pred_emotions = get_predictions(model_emotion_cause_classifier_restarts, X_test_conv)\n",
    "get_metrics_task1(y_test_pred_causes, y_test_pred_emotions, y_test_conv_causes, y_test_conv_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText models\n",
    "model_emotion_cause_classifier_restarts.load_weights(\"models/emotions_cause_default_fasttext.h5\")\n",
    "model_emotion_cause_classifier_default.load_weights(\"models/emotions_cause_default_fasttext.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_causes, y_val_pred_emotions = get_predictions(model_emotion_cause_classifier_default, X_validation_conv)\n",
    "get_metrics_task1(y_val_pred_causes, y_val_pred_emotions, y_validation_conv_causes, y_validation_conv_emotions)\n",
    "\n",
    "y_test_pred_causes, y_test_pred_emotions = get_predictions(model_emotion_cause_classifier_default, X_test_conv)\n",
    "get_metrics_task1(y_test_pred_causes, y_test_pred_emotions, y_test_conv_causes, y_test_conv_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_causes, y_val_pred_emotions = get_predictions(model_emotion_cause_classifier_restarts, X_validation_conv)\n",
    "get_metrics_task1(y_val_pred_causes, y_val_pred_emotions, y_validation_conv_causes, y_validation_conv_emotions)\n",
    "\n",
    "y_test_pred_causes, y_test_pred_emotions = get_predictions(model_emotion_cause_classifier_restarts, X_test_conv)\n",
    "get_metrics_task1(y_test_pred_causes, y_test_pred_emotions, y_test_conv_causes, y_test_conv_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d29a3c",
   "metadata": {},
   "source": [
    "### Pair classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset for the emotion-cause pair classifier\n",
    "def get_dataset_pairs(dataset_json, tokenizer, max_sequence_length):\n",
    "\trandom.seed(SEED)\n",
    "\tconversations_dataset = dataset_json['conversation']\n",
    "\tpairs_dataset = dataset_json['emotion-cause_pairs']\n",
    "\n",
    "\tX_pairs, y_pairs = [], []\n",
    "\temotion_cause_dicts = []\n",
    "\tfor conv_id, pairs in pairs_dataset.items():\n",
    "\t\temotion_cause_dict = {}\n",
    "\t\tconversation_data = conversations_dataset[conv_id]\n",
    "\t\t# Add the true pairs\n",
    "\t\tfor pair in pairs:\n",
    "\t\t\temotion, cause = pair\n",
    "\t\t\temotion_id = int(emotion.split(\"_\")[0])\n",
    "\t\t\tcause_id = int(cause.split(\"_\")[0])\n",
    "\t\t\t\n",
    "\t\t\tif emotion_id not in emotion_cause_dict:\n",
    "\t\t\t\temotion_cause_dict[emotion_id] = []\n",
    "\t\t\temotion_cause_dict[emotion_id].append(cause_id)\n",
    "\n",
    "\t\t\temotion_seq, cause_seq = None, None\n",
    "\t\t\tfor utterance_data in conversation_data:\n",
    "\t\t\t\tif utterance_data['utterance_ID'] == emotion_id:\n",
    "\t\t\t\t\temotion_seq = tokenizer.texts_to_sequences([cleanup_text(utterance_data['text'])])[0]\n",
    "\t\t\t\tif utterance_data['utterance_ID'] == cause_id:\n",
    "\t\t\t\t\tcause_seq = tokenizer.texts_to_sequences([cleanup_text(utterance_data['text'])])[0]\n",
    "\t\t\tif emotion_seq is not None and cause_seq is not None:\n",
    "\t\t\t\tpair_sequences = [emotion_seq, cause_seq]\n",
    "\t\t\t\tpair_sequences = pad_sequences(pair_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\t\t\t\tX_pairs.append(np.array(pair_sequences))\n",
    "\t\t\t\ty_pairs.append(1)\n",
    "\n",
    "\t\t# Add false pairs\n",
    "\t\tfor emotion_id in emotion_cause_dict.keys():\n",
    "\t\t\tavailable_cause_ids = set(range(1, len(conversation_data) + 1)) - set(emotion_cause_dict[emotion_id])\n",
    "\t\t\tif len(available_cause_ids) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tchosen_cause_id = random.choice(list(available_cause_ids))\n",
    "\n",
    "\t\t\temotion_seq, cause_seq = None, None\n",
    "\t\t\tfor utterance_data in conversation_data:\n",
    "\t\t\t\tif utterance_data['utterance_ID'] == emotion_id:\n",
    "\t\t\t\t\temotion_seq = tokenizer.texts_to_sequences([cleanup_text(utterance_data['text'])])[0]\n",
    "\t\t\t\tif utterance_data['utterance_ID'] == chosen_cause_id:\n",
    "\t\t\t\t\tcause_seq = tokenizer.texts_to_sequences([cleanup_text(utterance_data['text'])])[0]\n",
    "\t\t\tif emotion_seq is not None and cause_seq is not None:\n",
    "\t\t\t\tpair_sequences = [emotion_seq, cause_seq]\n",
    "\t\t\t\tpair_sequences = pad_sequences(pair_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\t\t\t\tX_pairs.append(np.array(pair_sequences))\n",
    "\t\t\t\ty_pairs.append(0)\n",
    "\t\temotion_cause_dicts.append(emotion_cause_dict)\n",
    "\n",
    "\tX_pairs, y_pairs = np.array(X_pairs), np.array(y_pairs)\n",
    "\tprint(X_pairs.shape)\n",
    "\tprint(y_pairs.shape)\n",
    "\n",
    "\treturn X_pairs, y_pairs, emotion_cause_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a dictionary to a list of pairs\n",
    "def convert_dict_to_pair_list(conversation_dict):\n",
    "\tpair_list = []\n",
    "\tfor emotion_id, cause_id_list in conversation_dict.items():\n",
    "\t\tpair_list.extend([(emotion_id - 1, cause_id - 1) for cause_id in cause_id_list])\n",
    "\treturn pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs_train_init, y_pairs_train_init, full_train_dicts = get_dataset_pairs(train_json, tokenizer_conversation, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs_train, X_pairs_val, y_pairs_train, y_pairs_val = train_test_split(X_pairs_train_init, y_pairs_train_init,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  test_size=0.1, random_state=SEED, shuffle=True)\n",
    "print(f\"X train shape = {X_pairs_train.shape}; y train shape = {y_pairs_train.shape}\")\n",
    "print(f\"X val shape = {X_pairs_val.shape}; y val shape = {y_pairs_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7490fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs_test, y_pairs_test, test_dicts = get_dataset_pairs(test_json, tokenizer_conversation, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21206166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair classifier (whether they are an emotion-cause pair)\n",
    "def get_emotion_cause_pair_classifier(lstm_hidden_dim, embedding_matrix, embedding_dropout, max_sequence_length):\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "    inputs = Input((2, max_sequence_length))\n",
    "    x = Embedding(input_dim=len(embedding_matrix), output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], mask_zero=True)(inputs)\n",
    "    x = tf.reshape(x, (-1, max_sequence_length, embedding_dim))\n",
    "    x = Dropout(embedding_dropout)(x)\n",
    "    x = LSTMEncoder(lstm_hidden_dim)(x)\n",
    "    # x shape is (batch, hidden_dim)\n",
    "    x = tf.reshape(x, (-1, 2 * lstm_hidden_dim))\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    # x shape is (batch, 2)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73164600",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_pairs = get_steps_per_epoch(X_pairs_train, BATCH_SIZE)\n",
    "print(steps_per_epoch_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_pair_model_default = get_emotion_cause_pair_classifier(100, embedding_matrix_word2vec, 0.2, max_sequence_length)\n",
    "compile_model(construct_pair_model_default, 100, steps_per_epoch_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(construct_pair_model_default, X_pairs_train, y_pairs_train, X_pairs_val, y_pairs_val, 100, 'classifier_pairs_default_word2vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_pair_model_default.load_weights(\"models/classifier_pairs_default_word2vec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_pair_model_restarts = get_emotion_cause_pair_classifier(100, embedding_matrix_word2vec, 0.2, max_sequence_length)\n",
    "compile_model(construct_pair_model_restarts, 100, steps_per_epoch_pairs, use_cosine_decay_restarts=True, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(construct_pair_model_restarts, X_pairs_train, y_pairs_train, X_pairs_val, y_pairs_val, 100, 'classifier_pairs_restarts_word2vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fdb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_pair_model_restarts.load_weights(\"models/classifier_pairs_restarts_word2vec.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8365517",
   "metadata": {},
   "source": [
    "### Pair classifier evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get potential pairs from the emotion and cause classifier (first model in the 2 step ECPE task)\n",
    "def get_potential_pairs(model_emotion_cause_classifier, X, indices, true_pair_dicts):\n",
    "\ty_pred_causes, y_pred_emotions = get_predictions(model_emotion_cause_classifier, X)\n",
    "    # Apply argmax along the last axis (assuming one-hot encoding)\n",
    "\targmax_result_emotions = np.argmax(y_pred_emotions, axis=-1)\n",
    "\n",
    "\tX_potential_pairs, y_potential_pairs = [], []\n",
    "\tfor i, conversation in enumerate(X):\n",
    "\t\tconversation_dict = true_pair_dicts[indices[i]]\n",
    "\t\tconversation_pairs_list = convert_dict_to_pair_list(conversation_dict)\n",
    "\n",
    "\t\t# Get list of emotion utterances\n",
    "\t\tresult_emotions_conv = argmax_result_emotions[i, :]\n",
    "\t\tpredicted_emotions = label_encoder_conversations.inverse_transform(result_emotions_conv)\n",
    "\n",
    "\t\ttext_sequences = tokenizer_conversation.sequences_to_texts(conversation)\n",
    "\t\tnon_empty_sequences = np.array([len(seq) > 0 for seq in text_sequences])\n",
    "\n",
    "\t\tfilter_condition_emotions = non_empty_sequences\n",
    "\t\temotion_utterances = conversation[filter_condition_emotions, :]\n",
    "\n",
    "\t\t# Get list of cause utterances\n",
    "\t\tresult_causes_conv = tf.squeeze(np.round(y_pred_causes[i, :]))\n",
    "\t\tis_cause_utterance = np.array(result_causes_conv == 1)\n",
    "\n",
    "\t\tfilter_condition_causes = is_cause_utterance & non_empty_sequences\n",
    "\t\tcause_utterances = conversation[filter_condition_causes]\n",
    "\n",
    "\t\tif emotion_utterances.shape[0] == 0 or cause_utterances.shape[0] == 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Apply cartesian product between the cause and emotion utterances\n",
    "\t\t# Only consider emotion utterances which are not neutral\n",
    "\t\tcartesian_product = list(itertools.product(np.arange(emotion_utterances.shape[0]), np.arange(cause_utterances.shape[0])))\n",
    "\t\tfor (emotion_id, cause_id) in cartesian_product:\n",
    "\t\t\temotion_utterance_pair = emotion_utterances[emotion_id, :]\n",
    "\t\t\tcause_uttearnce_pair = cause_utterances[cause_id, :]\n",
    "\t\t\tpotential_pair = np.array([emotion_utterance_pair, cause_uttearnce_pair])\n",
    "\n",
    "\t\t\tif predicted_emotions[emotion_id] != 'neutral':\n",
    "\t\t\t\tif (emotion_id, cause_id) in conversation_pairs_list:\n",
    "\t\t\t\t\ty_potential_pairs.append(1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ty_potential_pairs.append(0)\n",
    "\n",
    "\t\t\t\tX_potential_pairs.append(potential_pair)\n",
    "\n",
    "\tX_potential_pairs = np.array(X_potential_pairs)\n",
    "\tprint(X_potential_pairs.shape)\n",
    "\ty_potential_pairs = np.array(y_potential_pairs)\n",
    "\tprint(y_potential_pairs.shape)\n",
    "\t\n",
    "\treturn X_potential_pairs, y_potential_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_potential_pairs, y_val_potential_pairs = get_potential_pairs(model_emotion_cause_classifier_default, X_validation_conv, indices_conv_val, full_train_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_potential_pairs_restarts, y_val_potential_pairs_restarts = get_potential_pairs(model_emotion_cause_classifier_restarts, X_validation_conv, indices_conv_val, full_train_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd763725",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_potential_pairs, y_test_potential_pairs = get_potential_pairs(model_emotion_cause_classifier_default, X_test_conv, indices_conv_test, test_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e096c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_potential_pairs_restarts, y_test_potential_pairs_restarts = get_potential_pairs(model_emotion_cause_classifier_restarts, X_test_conv, indices_conv_test, test_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_pair_classifier_e2e(model_pair_classifier, X, y_true):\n",
    "\ty_pred = np.round(model_pair_classifier.predict(X, batch_size=16))\n",
    "\tprint(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics for the default model\")\n",
    "print(\"Metrics for validation set\")\n",
    "get_metrics_pair_classifier_e2e(construct_pair_model_default, X_val_potential_pairs, y_val_potential_pairs)\n",
    "\n",
    "print(\"Metrics for test set\")\n",
    "get_metrics_pair_classifier_e2e(construct_pair_model_default, X_test_potential_pairs, y_test_potential_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics for the model trained with cosine decay restarts\")\n",
    "print(\"Metrics for validation set\")\n",
    "get_metrics_pair_classifier_e2e(construct_pair_model_restarts, X_val_potential_pairs_restarts, y_val_potential_pairs_restarts)\n",
    "\n",
    "print(\"Metrics for test set\")\n",
    "get_metrics_pair_classifier_e2e(construct_pair_model_restarts, X_test_potential_pairs_restarts, y_test_potential_pairs_restarts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
